# Azure CLI Agent POC: Claude Code vs GitHub Copilot CLI

A structured proof of concept to evaluate AI coding assistants for Azure Infrastructure as Code (Bicep) development.

## Objective

Compare **Claude Code** and **GitHub Copilot CLI** across real-world Azure Landing Zone scenarios to determine:
- Which tool produces more accurate, deployable Bicep code
- Which requires fewer iterations to reach working solutions
- Which better understands existing codebases
- Which handles debugging and troubleshooting more effectively

## Repository Structure

```
azure-cli-agent-poc/
├── scenarios/                    # Test prompts for each phase
│   ├── phase1-greenfield/       # New IaC generation tasks
│   ├── phase2-existing-code/    # Codebase understanding tasks
│   ├── phase3-debugging/        # Troubleshooting tasks
│   └── phase4-workflow/         # Multi-tool/agentic tasks
├── modules/
│   ├── starter/                 # Pre-existing modules for Phase 2
│   └── solutions/
│       ├── claude-code/         # Solutions generated by Claude Code
│       └── copilot-cli/         # Solutions generated by Copilot CLI
├── infra/
│   ├── environments/
│   │   ├── dev/                 # Dev parameter files
│   │   └── prod/                # Prod parameter files
│   └── shared/                  # Shared types, functions
├── scripts/                     # Deployment & validation scripts
├── results/                     # Evaluation results & notes
└── .github/workflows/           # CI/CD for validation
```

## Test Protocol

### Before Starting

1. **Fresh sessions**: Start each tool with no prior context
2. **Identical prompts**: Use exact prompts from `scenarios/` folder
3. **Record everything**: Time, iterations, errors, observations

### For Each Task

```bash
# 1. Start timer
# 2. Paste prompt into tool
# 3. Record first output
# 4. Attempt deployment
az deployment sub what-if --location eastus2 --template-file <output.bicep>
# 5. If errors, iterate (record each iteration)
# 6. When working, record final time
# 7. Score using EVALUATION.md criteria
# 8. Save solution to modules/solutions/<tool>/
```

### Recording Results

Use `results/scorecard.md` template:
- Task ID, Tool, Time, Iterations, Score per criterion
- Notable observations (good patterns, mistakes, hallucinations)

## Quick Start

```bash
# Clone the repo
git clone <this-repo>
cd azure-cli-agent-poc

# Authenticate to Azure
az login
az account set --subscription <your-sub-id>

# Run Phase 1, Task 1.1 with Claude Code
# Open fresh Claude Code session, paste prompt from:
cat scenarios/phase1-greenfield/task-1.1-storage-account.md

# Run same task with Copilot CLI
# Open fresh Copilot CLI session, paste same prompt

# Save outputs
cp <claude-output> modules/solutions/claude-code/storage-account/
cp <copilot-output> modules/solutions/copilot-cli/storage-account/

# Validate both
./scripts/validate.sh modules/solutions/claude-code/storage-account/main.bicep
./scripts/validate.sh modules/solutions/copilot-cli/storage-account/main.bicep
```

## Test Phases

| Phase | Focus | Tasks |
|-------|-------|-------|
| 1 | Greenfield Generation | Create new Bicep modules from scratch |
| 2 | Existing Code | Understand and extend existing modules |
| 3 | Debugging | Fix broken deployments |
| 4 | Workflow | End-to-end with deployment & CI/CD |

See `scenarios/` for detailed prompts and `EVALUATION.md` for scoring criteria.

## Requirements

- Azure CLI 2.50+
- Bicep CLI 0.24+
- Azure subscription with Contributor access
- Claude Code CLI installed
- GitHub Copilot CLI installed (`gh extension install github/copilot-cli`)

## Evaluation Summary

After completing all tasks, aggregate scores in `results/summary.md` to determine:
1. Overall winner by total score
2. Winner by phase (which tool excels at what)
3. Recommendations for your team's workflows
